<div class="project-page">
    <h3>Academic Information</h3>
    
    <h4>Teaching Experience</h4>
    <div>
        <ul>
            <li>2014-2015 <b>Multimedia Systems</b></li>
            <li>2014-2015 <b>Robotics, Language and Planning</b></li>
            <li>2011-2012 <b>Programming Laboratory</b></li>
        </ul>
    </div>
    
    <h4>Supervised Undergraduate Projects</h4>
    <div>
        <ul>
            <li>2015-2016 <b>Joel Hernández Pla</b>: Implementación de algoritmos de desplazamiento para un robot Raspberry Pi</li>
            <li>2014-2015 <b>Arcadi Llanza</b>: Configuración de Eye-Tracking basado en Webcam: Implementación de un novedoso algoritmo de estimación de la mirada</li>
        </ul>
    </div>
    
    <h4>Publications</h4>
    
    <hr class="horizontal-separator" />
    <h5>Journal Articles</h5>
    
    <div class="publications-container">
        <!-- CIN PAPER February 2016 --> 
        <div class="publication">
            <div class="publication-main">
                <div class="publication-buttons">
                    <a href="#academic" class="publication-more">
                        <i class="glyphicon glyphicon-chevron-down"></i>
                    </a>
                    <a href="http://www.hindawi.com/journals/cin/2016/8680541/" class="tooltips" title="" target="_blank" data-original-title="External link">
                        <i class="glyphicon glyphicon-link"></i>
                    </a>
                    <a href="http://downloads.hindawi.com/journals/cin/2016/8680541.pdf" class="tooltips" title="" target="_blank" data-original-title="Download">
                        <i class="glyphicon glyphicon-download-alt"></i>
                    </a>
                </div>

                <h4 class="publication-title">Low Cost Eye Tracking: The Current Panorama</h4>
                <div class="publication-authors"><strong>Onur Ferhat</strong>, Fernando Vilari&ntilde;o</div>
                <div class="publication-conference">Computational Intelligence and Neuroscience, vol. 2016.</div>
            </div>
            <div class="publication-abstract">
                <h4>Abstract</h4>
                <p>Despite the availability of accurate, commercial gaze tracker devices working with infrared (IR) technology, visible light gaze tracking constitutes an interesting alternative by allowing scalability and removing hardware requirements. Over the last years, this field has seen examples of research showing performance comparable to the IR alternatives. In this work, we survey the previous work on remote, visible light gaze trackers and analyze the explored techniques from various perspectives such as calibration strategies, head pose invariance, and gaze estimation techniques. We also provide information on related aspects of research such as public datasets to test against, open source projects to build upon, and gaze tracking services to directly use in applications. With all this information, we aim to provide the contemporary and future researchers with a map detailing previously explored ideas and the required tools.</p>
            </div>
        </div>

        <!-- JEMR PAPER 2014 -->
        <div class="publication">
            <div class="publication-main">
                <div class="publication-buttons">
                    <a href="#academic" class="publication-more">
                        <i class="glyphicon glyphicon-chevron-down"></i>
                    </a>
                    <a href="http://www.jemr.org/online/7/3/2" class="tooltips" title="" target="_blank" data-original-title="External link">
                        <i class="glyphicon glyphicon-link"></i>
                    </a>
                    <a href="http://www.jemr.org/download/pictures/b4/dhlvuqzsse35e4wz0v3j3vn0co347y/ferhat_petmei_jemr_2014.pdf" class="tooltips" title="" target="_blank" data-original-title="Download">
                        <i class="glyphicon glyphicon-download-alt"></i>
                    </a>
                </div>

                <h4 class="publication-title">A cheap portable eye-tracker solution for common setups</h4>
                <div class="publication-authors"><strong>Onur Ferhat</strong>, Fernando Vilari&ntilde;o, F. Javier Sanchez</div>
                <div class="publication-conference">Journal of Eye Movement Research, 7(3), 1–10. 2014.</div>
            </div>
            <div class="publication-abstract">
                <h4>Abstract</h4>
                <p>We analyze the feasibility of a cheap eye-tracker where the hardware consists of a single webcam and a Raspberry Pi device. Our aim is to discover the limits of such a system and to see whether it provides an acceptable performance. We base our work on the open source Opengazer and we propose several improvements to create a robust, real-time system which can work on a computer with 30Hz sampling rate. After assessing the accuracy of our eye-tracker in elaborated experiments involving 12 subjects under 4 different system setups, we install it on a Raspberry Pi to create a portable stand-alone eye-tracker which achieves 1.42&deg; horizontal accuracy with 3Hz refresh rate for a building cost of 70 Euros.</p>
            </div>
        </div>
    </div>
    
    <hr class="horizontal-separator" />
    <h5>Conference Proceedings</h5>
    
    <div class="publications-container">
        <!-- SAGA PAPER October 2015 --> 
        <div class="publication">
            <div class="publication-main">
                <div class="publication-buttons">
                    <a href="#academic" class="publication-more">
                        <i class="glyphicon glyphicon-chevron-down"></i>
                    </a>
                    <a href="http://biecoll.ub.uni-bielefeld.de/volltexte/2015/5380/" class="tooltips" title="" target="_blank" data-original-title="External link">
                        <i class="glyphicon glyphicon-link"></i>
                    </a>
                    <a href="http://biecoll.ub.uni-bielefeld.de/volltexte/2015/5380/pdf/ferhat%20et%20al_Gaze%20interaction.pdf" class="tooltips" title="" target="_blank" data-original-title="Download">
                        <i class="glyphicon glyphicon-download-alt"></i>
                    </a>
                </div>

                <h4 class="publication-title">Gaze interaction for multi-display systems using natural light eye-tracker</h4>
                <div class="publication-authors"><strong>Onur Ferhat</strong>, Arcadi Llanza, Fernando Vilari&ntilde;o</div>
                <div class="publication-conference">In 2nd International Workshop on Solutions for Automatic Gaze Data Analysis (SAGA 2015).</div>
            </div>
            <div class="publication-abstract">
                <h4>Abstract</h4>
                <p>We propose a system implementing a novel feature based approach for webcam gaze interaction. Our system relies on the automated detection of the iris bounding box, followed by iris segmentation by means of binarisation. We develop this approach using the OpenGazer eye-tracker as a baseline.</p>
                <p>As a validation framework, we propose a two-display context-adaptive system which modifies in real time the data rendered in a secondary display depending on the particular object that is observed in the main display. We particularly tune this approach to Google-glass type devices. The application context of our research can be straightforwardly tested and extrapolated to different augmented reading experiences such as maps analysis, artworks detailed observation and music reading, among others.</p>
            </div>
        </div>
        
        <!-- ILI PAPER October 2015 --> 
        <div class="publication">
            <div class="publication-main">
                <div class="publication-buttons">
                    <a href="http://www.internet-librarian.com/2015/Tuesday.php#session_8994" class="tooltips" title="" target="_blank" data-original-title="External link">
                        <i class="glyphicon glyphicon-link"></i>
                    </a>
                </div>

                <h4 class="publication-title">Creative Engagement in Digital Collections</h4>
                <div class="publication-authors">Dan Norton, Fernando Vilari&ntilde;o, <strong>Onur Ferhat</strong></div>
                <div class="publication-conference">In Internet Librarian International. 2015.</div>
            </div>
        </div>
        
        <!-- ISEA PAPER August 2015 --> 
        <div class="publication">
            <div class="publication-main">
                <div class="publication-buttons">
                    <a href="#academic" class="publication-more">
                        <i class="glyphicon glyphicon-chevron-down"></i>
                    </a>
                </div>

                <h4 class="publication-title">Memory Fields: DJs in the Library</h4>
                <div class="publication-authors">Dan Norton, Fernando Vilari&ntilde;o, <strong>Onur Ferhat</strong></div>
                <div class="publication-conference">In 21st International Symposium on Electronic Art (accepted). 2015</div>
            </div>
            <div class="publication-abstract">
                <h4>Abstract</h4>
                <p>A Disc Jockey's (DJ's) model of information interaction is valuable for creative engagement with digital collections of all kinds. Selecting and mixing are fundamental creative behaviours. This paper presents an artwork derived from the DJ’s model, which is a dual screen interface used to access a public library’s digital collections.</p>
            </div>
        </div>

        <!-- IbPRIA PAPER June 2015 -->
        <div class="publication">
            <div class="publication-main">
                <div class="publication-buttons">
                    <a href="#academic" class="publication-more">
                        <i class="glyphicon glyphicon-chevron-down"></i>
                    </a>
                    <a href="http://link.springer.com/chapter/10.1007%2F978-3-319-19390-8_64" class="tooltips" title="" target="_blank" data-original-title="External link">
                        <i class="glyphicon glyphicon-link"></i>
                    </a>
                    <!-- <a href="#TODO" class="tooltips" title="" target="_blank" data-original-title="Download">
                        <i class="glyphicon glyphicon-download-alt"></i>
                    </a>-->
                </div>

                <h4 class="publication-title">A Feature-Based Gaze Estimation Algorithm for Natural Light Scenarios</h4>
                <div class="publication-authors"><strong>Onur Ferhat</strong>, Arcadi Llanza, Fernando Vilari&ntilde;o</div>
                <div class="publication-conference">In 7th Iberian Conference on Pattern Recognition and Image Analysis (Vol. 9117, pp. 569–576). June 2015. Springer International Publishing.</div>
            </div>
            <div class="publication-abstract">
                <h4>Abstract</h4>
                <p>We present an eye tracking system that works with regular webcams. We base our work on open source CVC Eye Tracker and we propose a number of improvements and a novel gaze estimation method. The new method uses features extracted from iris segmentation and it does not fall into the traditional categorization of appearance–based/model–based methods. Our experiments show that our approach reduces the gaze estimation errors by 34 % in the horizontal direction and by 12% in the vertical direction compared to the baseline system.</p>
            </div>
        </div>

        <!-- PETMEI PAPER 2013 -->
        <div class="publication">
            <div class="publication-main">
                <div class="publication-buttons">
                    <a href="#academic" class="publication-more">
                        <i class="glyphicon glyphicon-chevron-down"></i>
                    </a>
                    <a href="http://2013.petmei.org/program.html" class="tooltips" title="" target="_blank" data-original-title="External link">
                        <i class="glyphicon glyphicon-link"></i>
                    </a>
                    <a href="http://refbase.cvc.uab.es/files/FeV2013a.pdf" class="tooltips" title="" target="_blank" data-original-title="Download">
                        <i class="glyphicon glyphicon-download-alt"></i>
                    </a>
                </div>

                <h4 class="publication-title">A cheap portable eye-tracker solution for common setups</h4>
                <div class="publication-authors"><strong>Onur Ferhat</strong>, Fernando Vilari&ntilde;o</div>
                <div class="publication-conference">In 17th European Conference on Eye Movements, PETMEI Workshop. 2013.</div>
            </div>
            <div class="publication-abstract">
                <h4>Abstract</h4>
                <p>We analyze the feasibility of a cheap eye-tracker where the hardware consists of a single webcam and a Raspberry Pi device. Our aim is to discover the limits of such a system and to see whether it provides an acceptable performance. We base our work on the open source Opengazer and we propose several improvements to create a robust, real-time system. After assessing the accuracy of our eye-tracker in elaborated experiments involving 18 subjects under 4 different system setups, we developed a simple game to see how it performs in practice and we also installed it on a Raspberry Pi to create a portable stand-alone eye-tracker which achieves 1.62&deg; horizontal accuracy with 3 fps refresh rate for a building cost of 70 Euros.</p>
            </div>
        </div>

    </div>    
    
    <hr class="horizontal-separator" />
    <h5>Doctoral Thesis</h5>
    
    <div class="publications-container">
        <!-- DOCTORAL THESIS 2017 -->
        <div class="publication">
            <div class="publication-main">
                <div class="publication-buttons">
                    <a href="#academic" class="publication-more">
                        <i class="glyphicon glyphicon-chevron-down"></i>
                    </a>
                    <!--<a href="#TODO" class="tooltips" title="" target="_blank" data-original-title="External link">
                        <i class="glyphicon glyphicon-link"></i>
                    </a>-->
                    <a href="http://www.tdx.cat/bitstream/handle/10803/454998/onfe1de1.pdf?sequence=1" class="tooltips" title="" target="_blank" data-original-title="Download">
                        <i class="glyphicon glyphicon-download-alt"></i>
                    </a>
                </div>

                <h4 class="publication-title">Analysis of Head-Pose Invariant, Natural Light Gaze Estimation Methods</h4>
                <div class="publication-authors"><strong>Onur Ferhat</strong></div>
                <div class="publication-conference">Department of Computer Science, Universitat Autonoma de Barcelona. September 2017.</div>
            </div>
            <div class="publication-abstract">
                <h4>Abstract</h4>
                <p>Eye tracker devices have traditionally been only used inside laboratories, requiring trained professionals and elaborate setup mechanisms. However, in the recent years the scientific work on easier–to–use eye trackers which require no special hardware—other than the omnipresent front facing cameras in computers, tablets, and mobiles—is aiming at making this technology common–place. These types of trackers have several extra challenges that make the problem harder, such as low resolution images provided by a regular webcam, the changing ambient lighting conditions, personal appearance differences, changes in head pose, and so on. Recent research in the field has focused on all these challenges in order to provide better gaze estimation performances in a real world setup.</p>
                <p>In this work, we aim at tackling the gaze tracking problem in a single camera setup. We first analyze all the previous work in the field, identifying the strengths and weaknesses of each tried idea. We start our work on the gaze tracker with an appearance–based gaze estimation method, which is the simplest idea that creates a direct mapping between a rectangular image patch extracted around the eye in a camera image, and the gaze point (or gaze direction). Here, we do an extensive analysis of the factors that affect the performance of this tracker in several experimental setups, in order to address these problems in future works. In the second part of our work, we propose a feature–based gaze estimation method, which encodes the eye region image into a compact representation. We argue that this type of representation is better suited to dealing with head pose and lighting condition changes, as it both reduces the dimensionality of the input (i.e. eye image) and breaks the direct connection between image pixel intensities and the gaze estimation. Lastly, we use a face alignment algorithm to have robust face pose estimation, using a 3D model customized to the subject using the tracker. We combine this with a convolutional neural network trained on a large dataset of images to build a face pose invariant gaze tracker.</p>
            </div>
        </div>
    </div>    
    
    <hr class="horizontal-separator" />
    <h5>Master's Thesis</h5>
    
    <div class="publications-container">
        <!-- MASTER THESIS 2012 -->
        <div class="publication">
            <div class="publication-main">
                <div class="publication-buttons">
                    <a href="#academic" class="publication-more">
                        <i class="glyphicon glyphicon-chevron-down"></i>
                    </a>
                    <!--<a href="#TODO" class="tooltips" title="" target="_blank" data-original-title="External link">
                        <i class="glyphicon glyphicon-link"></i>
                    </a>-->
                    <a href="http://refbase.cvc.uab.es/files/Fer2012.pdf" class="tooltips" title="" target="_blank" data-original-title="Download">
                        <i class="glyphicon glyphicon-download-alt"></i>
                    </a>
                </div>

                <h4 class="publication-title">Eye-Tracking with Webcam-Based Setups: Implementation of a Real-Time System and an Analysis of Factors Affecting Performance</h4>
                <div class="publication-authors"><strong>Onur Ferhat</strong></div>
                <div class="publication-conference">Department of Computer Science, Universitat Autonoma de Barcelona. September 2012.</div>
            </div>
            <div class="publication-abstract">
                <h4>Abstract</h4>
                <p>In the recent years commercial eye-tracking hardware has become more common, with the introduction of new models from several brands that have better performance and easier setup procedures. A cause and at the same time a result of this phenomenon is the popularity of eye-tracking research directed at marketing, accessibility and usability, among others. One problem with these hardware components is scalability, because both the price and the necessary expertise to operate them makes it practically impossible in the large scale. In this work, we analyze the feasibility of a software eye-tracking system based on a single, ordinary webcam. Our aim is to discover the limits of such a system and to see whether it provides acceptable performances. The significance of this setup is that it is the most common setup found in consumer environments, off-the-shelf electronic devices such as laptops, mobile phones and tablet computers. As no special equipment such as infrared lights, mirrors or zoom lenses are used; setting up and calibrating the system is easier compared to other approaches using these components. Our work is based on the open source application Opengazer, which provides a good starting point for our contributions. We propose several improvements in order to push the system’s performance further and make it feasible as a robust, real-time device. Then we carry out an elaborate experiment involving 18 human subjects and 4 different system setups. Finally, we give an analysis of the results and discuss the effects of setup changes, subject differences and modifications in the software.</p>
            </div>
        </div>
    </div>
</div>